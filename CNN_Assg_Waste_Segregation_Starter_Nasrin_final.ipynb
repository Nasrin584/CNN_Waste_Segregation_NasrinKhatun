{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lf5lYawIw8tE"
   },
   "source": [
    "# **Waste Material Segregation for Improving Waste Management**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY1InIbkw80B"
   },
   "source": [
    "## **Objective**\n",
    "\n",
    "The objective of this project is to implement an effective waste material segregation system using convolutional neural networks (CNNs) that categorises waste into distinct groups. This process enhances recycling efficiency, minimises environmental pollution, and promotes sustainable waste management practices.\n",
    "\n",
    "The key goals are:\n",
    "\n",
    "* Accurately classify waste materials into categories like cardboard, glass, paper, and plastic.\n",
    "* Improve waste segregation efficiency to support recycling and reduce landfill waste.\n",
    "* Understand the properties of different waste materials to optimise sorting methods for sustainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZGTCfyUxalZ"
   },
   "source": [
    "## **Data Understanding**\n",
    "\n",
    "The Dataset consists of images of some common waste materials.\n",
    "\n",
    "1. Food Waste\n",
    "2. Metal\n",
    "3. Paper\n",
    "4. Plastic\n",
    "5. Other\n",
    "6. Cardboard\n",
    "7. Glass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZJtmMnzQjAr"
   },
   "source": [
    "**Data Description**\n",
    "\n",
    "* The dataset consists of multiple folders, each representing a specific class, such as `Cardboard`, `Food_Waste`, and `Metal`.\n",
    "* Within each folder, there are images of objects that belong to that category.\n",
    "* However, these items are not further subcategorised. <br> For instance, the `Food_Waste` folder may contain images of items like coffee grounds, teabags, and fruit peels, without explicitly stating that they are actually coffee grounds or teabags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBFt43WDzWSJ"
   },
   "source": [
    "## **1. Load the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfy0rjJ1yzFl"
   },
   "source": [
    "Load and unzip the dataset zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N35LLuWXzUQH"
   },
   "source": [
    "**Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "DmZo7m1-J_Ou"
   },
   "outputs": [],
   "source": [
    "# Recommended versions:\n",
    "\n",
    "#numpy version: 1.26.4\n",
    "# pandas version: 2.2.2\n",
    "# seaborn version: 0.13.2\n",
    "# matplotlib version: 3.10.0\n",
    "# PIL version: 11.1.0\n",
    "# tensorflow version: 2.18.0\n",
    "# keras version: 3.8.0\n",
    "# sklearn version: 1.6.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install numpy==1.26.4 pandas==2.2.2 seaborn==0.13.2 Pillow==11.1.0 tensorflow==2.18.0 keras==3.8.0 scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "dzM50pygphUe"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNAzJi1c9WAX"
   },
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "TM1qn2DKtjR6"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9404\\2029001438.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mzip_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Dataset extracted to: {extract_dir}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[1;34m(self, path, members, pwd)\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[1;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[0;32m   1695\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m              \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1697\u001b[1;33m             \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    990\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    742\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[0;32m    743\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load and unzip the dataset\n",
    "import zipfile\n",
    "\n",
    "zip_path = r\"F:\\\\DS C71\\\\Waste_Management_Case_Study\\data.zip\"\n",
    "\n",
    "extract_dir = r\"F:\\\\DS C71\\\\Waste_Management_Case_Study\\data\"\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "    \n",
    "print(f\"Dataset extracted to: {extract_dir}\")\n",
    "\n",
    "data_dir = os.path.join(extract_dir, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(extract_dir, \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDp_EWxVOhUu"
   },
   "source": [
    "## **2. Data Preparation** <font color=red> [25 marks] </font><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7Ac8VxvjWnw"
   },
   "source": [
    "### **2.1 Load and Preprocess Images** <font color=red> [8 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghmtINrMXDMy"
   },
   "source": [
    "Let us create a function to load the images first. We can then directly use this function while loading images of the different categories to load and crop them in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZQ1UZNfQCWX"
   },
   "source": [
    "#### **2.1.1** <font color=red> [3 marks] </font><br>\n",
    "Create a function to load the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6klNk9rcAtr"
   },
   "outputs": [],
   "source": [
    "# Create a function to load the raw images\n",
    "\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(data_dir, image_size=(150, 150)):\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    for label_index, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize(image_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(label_index)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {image_path}: {e}\")\n",
    "    return np.array(X), np.array(y), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, class_names = load_images_from_folder(data_dir)\n",
    "print(\"Loaded images:\", X.shape)\n",
    "print(\"Loaded labels:\", y.shape)\n",
    "print(\"Unique Labels: \", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J01VQrLhQsxx"
   },
   "source": [
    "#### **2.1.2** <font color=red> [5 marks] </font><br>\n",
    "Load images and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C9Oo0PTtYLf"
   },
   "source": [
    "Load the images from the dataset directory. Labels of images are present in the subdirectories.\n",
    "\n",
    "Verify if the images and labels are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zm2zlZbmamzy"
   },
   "outputs": [],
   "source": [
    "# Get the images and their labels\n",
    "plt.figure(figsize=(15, 6))\n",
    "shown = set()\n",
    "for idx in range(len(X)):\n",
    "    if y[idx] not in shown:\n",
    "        plt.subplot(1, len(class_names), y[idx] + 1)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.title(class_names[y[idx]])\n",
    "        plt.axis('off')\n",
    "        shown.add(y[idx])\n",
    "        if len(shown) == len(class_names):\n",
    "            break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(y)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([class_names[i] for i in counts.keys()], counts.values(), color='skyblue')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()       # check if labels look consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y)\n",
    "for label_idx, count in label_counts.items():\n",
    "    print(f\"{class_names[label_idx]}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "shown = set()\n",
    "for idx in range(len(X)):\n",
    "    if y[idx] not in shown:\n",
    "        plt.subplot(1, len(class_names), y[idx] + 1)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.title(class_names[y[idx]])\n",
    "        plt.axis('off')\n",
    "        shown.add(y[idx])\n",
    "        if len(shown) == len(class_names):\n",
    "            break\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26Is-EwKuyGf"
   },
   "source": [
    "Perform any operations, if needed, on the images and labels to get them into the desired format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I64rs77bkAYk"
   },
   "source": [
    "### **2.2 Data Visualisation** <font color=red> [9 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCAepbyAQdI2"
   },
   "source": [
    "#### **2.2.1** <font color=red> [3 marks] </font><br>\n",
    "Create a bar plot to display the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm5LuWSFqTac"
   },
   "outputs": [],
   "source": [
    "# Visualise Data Distribution\n",
    "\n",
    "# Count the number of samples per class\n",
    "class_counts = np.bincount(y)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(class_names, class_counts, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"Class Distribution of Waste Images\", fontsize=14)\n",
    "plt.xlabel(\"Waste Material Class\", fontsize=12)\n",
    "plt.ylabel(\"Number of Images\", fontsize=12)\n",
    "\n",
    "# Annotate each bar with the count\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 10, str(height), ha='center', va='bottom')\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNWsPfTzRh7x"
   },
   "source": [
    "#### **2.2.2** <font color=red> [3 marks] </font><br>\n",
    "Visualise some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37yXZzfLyOWt"
   },
   "outputs": [],
   "source": [
    "# Visualise Sample Images (across different labels)\n",
    "\n",
    "samples_per_class = 5\n",
    "num_classes = len(class_names)\n",
    "plt.figure(figsize=(samples_per_class * 3, num_classes * 3))\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    class_name = class_names[class_idx]\n",
    "    indices = np.where(y == class_idx)[0][:samples_per_class]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt_idx = class_idx * samples_per_class + i + 1\n",
    "        plt.subplot(num_classes, samples_per_class, plt_idx)\n",
    "        plt.imshow(X[idx].astype('uint8'))\n",
    "        plt.title(class_name if i == 0 else \"\")\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Sample Images per Class\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrxdFzNigaYG"
   },
   "source": [
    "#### **2.2.3** <font color=red> [3 marks] </font><br>\n",
    "Based on the smallest and largest image dimensions, resize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyVvjNXqgIGe"
   },
   "outputs": [],
   "source": [
    "# Find the smallest and largest image dimensions from the data set\n",
    "\n",
    "sizes = []\n",
    "for class_name in class_names:\n",
    "    for img_name in os.listdir(os.path.join(data_dir, class_name)):\n",
    "        try:\n",
    "            with Image.open(os.path.join(data_dir, class_name, img_name)) as img:\n",
    "                sizes.append(img.size)\n",
    "        except:\n",
    "            continue\n",
    "widths, heights = zip(*sizes)\n",
    "print(f\"Smallest Dimension: {min(widths)} x {min(heights)}\")\n",
    "print(f\"Largest Dimension: {max(widths)} x {max(heights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fz7EutUrgKFZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Resize the image dimensions\n",
    "\n",
    "resized_images = []\n",
    "\n",
    "for img in X:\n",
    "    resized = cv2.resize(img, (256, 256))  # This won't change anything if already 256x256\n",
    "    resized_images.append(resized)\n",
    "\n",
    "X = np.array(resized_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCB8uOckR5li"
   },
   "source": [
    "### **2.3 Encoding the classes** <font color=red> [3 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdC4dpTWt9eo"
   },
   "source": [
    "There are seven classes present in the data.\n",
    "\n",
    "We have extracted the images and their labels, and visualised their distribution. Now, we need to perform encoding on the labels. Encode the labels suitably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Nwd0Ztvkf7K"
   },
   "source": [
    "#### **2.3.1** <font color=red> [3 marks] </font><br>\n",
    "Encode the target class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "qkyXDQN-660s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels:  ['Cardboard', 'Food_Waste', 'Glass', 'Metal', 'Other', 'Paper', 'Plastic']\n",
      "Original class labels: [0 1 2 3 4 5 6]\n",
      "Encoded labels:\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels suitably\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "unique = np.unique(y)\n",
    "classes = [class_names[label] for label in unique]\n",
    "print(\"Unique Labels: \", classes)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_cat = to_categorical(y_encoded)\n",
    "\n",
    "# Check the results\n",
    "print(\"Original class labels:\", le.classes_)\n",
    "print(\"Encoded labels:\\n\", y_cat)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_encoded = to_categorical(y, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNBM4hsuSaoj"
   },
   "source": [
    "### **2.4 Data Splitting** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0xw-Qlh29cZ"
   },
   "source": [
    "#### **2.4.1** <font color=red> [5 marks] </font><br>\n",
    "Split the dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "TErpx_JOkwjO"
   },
   "outputs": [],
   "source": [
    "# Assign specified parts of the dataset to train and validation sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.astype('float16') / 255.0\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mILXPeY-X-zP"
   },
   "source": [
    "## **3. Model Building and Evaluation** <font color=red> [20 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4E0afHwy5M_i"
   },
   "source": [
    "### **3.1 Model building and training** <font color=red> [15 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsu8K3tL5a5Q"
   },
   "source": [
    "#### **3.1.1** <font color=red> [10 marks] </font><br>\n",
    "Build and compile the model. Use 3 convolutional layers. Add suitable normalisation, dropout, and fully connected layers to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awW9V2lmMK_d"
   },
   "source": [
    "Test out different configurations and report the results in conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "num_classes = y_encoded.shape[1]\n",
    "input_shape = X.shape[1:]\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001), input_shape=input_shape),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t4duT1wX5wS"
   },
   "source": [
    "#### **3.1.2** <font color=red> [5 marks] </font><br>\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcrEzo51Qj6w"
   },
   "source": [
    "Use appropriate metrics and callbacks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7Ut0BicH_I8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m101/191\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7:14\u001b[0m 5s/step - accuracy: 0.2630 - loss: 26.6278"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation data\n",
    "\n",
    "train_acc = history.history['accuracy'][-1]\n",
    "print(f\"Final Training Accuracy: {train_acc:.2f}\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Val')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Val')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWT-bIj9YVzh"
   },
   "source": [
    "### **3.2 Model Testing and Evaluation** <font color=red> [5 marks] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjhU3i5v59d6"
   },
   "source": [
    "#### **3.2.1** <font color=red> [5 marks] </font><br>\n",
    "Evaluate the model on test dataset. Derive appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_MtfUM_4y7j"
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set; display suitable metrics\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get predictions\n",
    "y_pred_probs = model.predict(X_val)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # predicted class indices\n",
    "y_true = np.argmax(y_val, axis=1)         # true class indices if one-hot encoded\n",
    "\n",
    "# Step 2: Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "# Step 3: Print results\n",
    "print(f\"Accuracy       : {accuracy:.2f}\")\n",
    "print(f\"Precision      : {precision:.2f}\")\n",
    "print(f\"Recall         : {recall:.2f}\")\n",
    "print(f\"F1-Score       : {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "labels = ['Cardboard', 'Food_Waste', 'Glass', 'Metal', 'Other', 'Paper', 'Plastic']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utro5JdHS0JM"
   },
   "source": [
    "## **4. Data Augmentation** <font color=red> [optional] </font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T6QlG4eS4xi"
   },
   "source": [
    "#### **4.1 Create a Data Augmentation Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AXlfuoa4jQV"
   },
   "source": [
    "##### **4.1.1**\n",
    "Define augmentation steps for the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbHCwkX0dq0R"
   },
   "outputs": [],
   "source": [
    "# Define augmentation steps to augment images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define augmentation steps for training data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,               # Normalize pixel values\n",
    "    rotation_range=20,            # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.1,        # Shift width by 10%\n",
    "    height_shift_range=0.1,       # Shift height by 10%\n",
    "    shear_range=0.1,              # Shear angle\n",
    "    zoom_range=0.2,               # Zoom in/out by up to 20%\n",
    "    horizontal_flip=True,         # Flip images horizontally\n",
    "    fill_mode='nearest'           # Fill in missing pixels\n",
    ")\n",
    "\n",
    "# For validation data, only rescale\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07i11vgMEmM2"
   },
   "source": [
    "Augment and resample the images.\n",
    "In case of class imbalance, you can also perform adequate undersampling on the majority class and augment those images to ensure consistency in the input datasets for both classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chvmgE2r4xPZ"
   },
   "source": [
    "Augment the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-JBheeYFS8d"
   },
   "outputs": [],
   "source": [
    "# Create a function to augment the images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "def augment_images(X, y, augment_count=2):\n",
    "    \"\"\"\n",
    "    Augment each image `augment_count` times using basic transformations.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.array of images (4D: samples, height, width, channels)\n",
    "    - y: corresponding labels (1D: integers or strings)\n",
    "    - augment_count: number of augmented copies to create per image\n",
    "\n",
    "    Returns:\n",
    "    - X_aug: augmented images\n",
    "    - y_aug: corresponding labels\n",
    "    \"\"\"\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    X_aug, y_aug = [], []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        x = X[i].reshape((1,) + X[i].shape)  # reshape for ImageDataGenerator\n",
    "        label = y[i]\n",
    "        gen = datagen.flow(x, batch_size=1)\n",
    "        for _ in range(augment_count):\n",
    "            aug_img = next(gen)[0]\n",
    "            X_aug.append(aug_img)\n",
    "            y_aug.append(label)\n",
    "\n",
    "    return np.array(X_aug), np.array(y_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = X_aug.astype('float32')\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_aug, y_aug = augment_images(X_train, y_train, augment_count=1)\n",
    "\n",
    "# Combine original + augmented data\n",
    "X_train_combined = np.concatenate([X_train, X_aug])\n",
    "y_train_combined = np.concatenate([y_train, y_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ddy1y1nPIlvM"
   },
   "outputs": [],
   "source": [
    "# Create the augmented training dataset\n",
    "\n",
    "def augment_images(X, y, augment_count=2):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    X_aug, y_aug = [], []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x = X[i].reshape((1,) + X[i].shape)\n",
    "        label = y[i]\n",
    "        gen = datagen.flow(x, batch_size=1)\n",
    "        for _ in range(augment_count):\n",
    "            aug_img = next(gen)[0]\n",
    "            X_aug.append(aug_img)\n",
    "            y_aug.append(label)\n",
    "\n",
    "    return np.array(X_aug), np.array(y_aug)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug, y_aug = augment_images(X_train, y_train, augment_count=2)  # Augment each image 2 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = np.concatenate([X_train, X_aug])\n",
    "y_train_combined = np.concatenate([y_train, y_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train_combined, y_train_combined = shuffle(X_train_combined, y_train_combined, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train_combined, y_train_combined, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZYekkw9TCvP"
   },
   "source": [
    "##### **4.1.2**\n",
    "\n",
    "Train the model on the new augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBcRbt57FEct"
   },
   "outputs": [],
   "source": [
    "# Train the model using augmented images\n",
    "\n",
    "model.fit(train_generator, validation_data=(X_val, y_val), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFPuXAvHkJVz"
   },
   "source": [
    "## **5. Conclusions** <font color = red> [5 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33tWCHjpO5hH"
   },
   "source": [
    "#### **5.1 Conclude with outcomes and insights gained** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3e1TLo2kWi0"
   },
   "source": [
    "* Report your findings about the data\n",
    "* Report model training results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1KW2JSuqBLb3DdmqZSAHtN2K0gX8C2HcV",
     "timestamp": 1740722968634
    },
    {
     "file_id": "1XXsgvgvRpr1OqI_K70kBWgfsI9bByK3r",
     "timestamp": 1738303842187
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
